use anyhow::Result;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use std::sync::atomic::{AtomicBool, Ordering};
use tokio::sync::RwLock;
use uuid::Uuid;

use crate::core::ai_manager::AIManager;
use crate::core::git_engine::GitEngine;
use crate::core::conversation_logger::StepInfo;
use crate::core::ai_provider::{AIRequest, ChatMessage};
use crate::core::prompt_manager::{PromptManager, CommitContext};
use crate::utils::token_counter::TokenCounter;

/**
 * åˆ†å±‚æäº¤ç®¡ç†å™¨
 * ä½œè€…ï¼šEvilek
 * ç¼–å†™æ—¥æœŸï¼š2025-08-04
 */

#[derive(Debug, Clone)]
pub struct LayeredCommitProgress {
    pub session_id: String,
    pub current_step: u32,
    pub total_steps: u32,
    pub current_file: Option<String>,
    pub status: String,
    pub file_summaries: Vec<FileSummary>,
    /// AIå®æ—¶è¾“å‡ºå†…å®¹ - Author: Evilek, Date: 2025-01-10
    pub ai_stream_content: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FileSummary {
    #[serde(rename = "filePath")]
    pub file_path: String,
    pub summary: String,
    #[serde(rename = "tokensUsed")]
    pub tokens_used: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LayeredCommitResult {
    #[serde(rename = "sessionId")]
    pub session_id: String,
    #[serde(rename = "finalMessage")]
    pub final_message: String,
    #[serde(rename = "fileSummaries")]
    pub file_summaries: Vec<FileSummary>,
    #[serde(rename = "totalProcessingTimeMs")]
    pub total_processing_time_ms: u64,
    #[serde(rename = "conversationRecords")]
    pub conversation_records: Vec<String>, // è®°å½•IDåˆ—è¡¨
    /// æ¨ç†å†…å®¹ï¼ˆ<think>æ ‡ç­¾å†…çš„å†…å®¹ï¼‰
    /// ä½œè€…ï¼šEvilek
    /// ç¼–å†™æ—¥æœŸï¼š2025-01-10
    #[serde(rename = "reasoningContent")]
    pub reasoning_content: Option<String>,
}

pub struct LayeredCommitManager {
    ai_manager: Arc<RwLock<AIManager>>,
    git_engine: Arc<RwLock<GitEngine>>,
    cancelled: Arc<AtomicBool>, // ä»»åŠ¡å–æ¶ˆæ ‡å¿—
}

impl LayeredCommitManager {
    pub fn new(
        ai_manager: Arc<RwLock<AIManager>>,
        git_engine: Arc<RwLock<GitEngine>>,
    ) -> Self {
        Self {
            ai_manager,
            git_engine,
            cancelled: Arc::new(AtomicBool::new(false)),
        }
    }

    /// å–æ¶ˆå½“å‰ä»»åŠ¡
    /// Author: Evilek, Date: 2025-01-09
    pub fn cancel(&self) {
        self.cancelled.store(true, Ordering::Relaxed);
    }

    /// æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
    /// Author: Evilek, Date: 2025-01-09
    fn is_cancelled(&self) -> bool {
        self.cancelled.load(Ordering::Relaxed)
    }

    /// æ£€æŸ¥æ˜¯å¦éœ€è¦å¯ç”¨åˆ†å±‚æäº¤
    pub async fn should_use_layered_commit(
        &self,
        _template_id: &str,
        diff_content: &str,
        _staged_files: &[String],
    ) -> Result<bool> {
        let ai_manager = self.ai_manager.read().await;
        let config = ai_manager.get_config().await;
        
        // æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†åˆ†å±‚æäº¤åŠŸèƒ½
        if !config.features.enable_layered_commit {
            return Ok(false);
        }

        // è·å–å½“å‰æ¨¡å‹çš„tokené™åˆ¶
        let model_max_tokens = self.get_model_max_tokens(&config.base.model).await?;
        
        // æ„å»ºåŸºæœ¬çš„æ¶ˆæ¯æ¥ä¼°ç®—tokenæ•°é‡
        let system_message = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Gitæäº¤æ¶ˆæ¯ç”ŸæˆåŠ©æ‰‹ã€‚";
        let user_message = format!("è¯·ä¸ºä»¥ä¸‹ä»£ç å˜æ›´ç”Ÿæˆæäº¤æ¶ˆæ¯:\n{}", diff_content);
        
        let messages = vec![
            ChatMessage {
                role: "system".to_string(),
                content: system_message.to_string(),
            },
            ChatMessage {
                role: "user".to_string(),
                content: user_message,
            },
        ];
        
        let request = AIRequest {
            messages,
            model: config.base.model.clone(),
            temperature: Some(0.3),
            max_tokens: Some(config.advanced.max_tokens), // ä½¿ç”¨ç³»ç»Ÿå…¨å±€é…ç½®çš„max_tokensï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 
            stream: Some(false),
        };

        let estimated_tokens = TokenCounter::estimate_request_tokens(&request);
        let is_over_limit = TokenCounter::is_over_limit(estimated_tokens, model_max_tokens);

        Ok(is_over_limit)
    }

    /// æ‰§è¡Œåˆ†å±‚æäº¤
    pub async fn execute_layered_commit<F>(
        &self,
        template_id: &str,
        staged_files: Vec<String>,
        branch_name: Option<String>,
        repository_path: Option<String>,
        progress_callback: F,
    ) -> Result<LayeredCommitResult>
    where
        F: Fn(LayeredCommitProgress) + Send + Sync,
    {
        let session_id = Uuid::new_v4().to_string();
        let start_time = std::time::Instant::now();

        // ç¬¬ä¸€æ­¥ï¼šè·å–æ¯ä¸ªæ–‡ä»¶çš„diff
        let files_with_diffs = self.get_files_with_diffs(&staged_files, template_id).await?;
        let total_files = files_with_diffs.len();

        // åˆå§‹åŒ–è¿›åº¦
        let mut progress = LayeredCommitProgress {
            session_id: session_id.clone(),
            current_step: 0,
            total_steps: (total_files + 1) as u32, // æ–‡ä»¶åˆ†æ + æœ€ç»ˆæ€»ç»“
            current_file: None,
            status: "å¼€å§‹åˆ†å±‚æäº¤".to_string(),
            file_summaries: Vec::new(),
            ai_stream_content: None,  // åˆå§‹åŒ–AIæµå¼è¾“å‡ºå†…å®¹ - Author: Evilek, Date: 2025-01-10
        };
        progress_callback(progress.clone());

        // ç¬¬äºŒæ­¥ï¼šä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆæ‘˜è¦
        let mut file_summaries = Vec::new();
        let mut conversation_records = Vec::new();

        for (index, (file_path, diff_content)) in files_with_diffs.iter().enumerate() {
            // æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ - Author: Evilek, Date: 2025-01-09
            if self.is_cancelled() {
                return Err(anyhow::anyhow!("åˆ†å±‚æäº¤å·²è¢«ç”¨æˆ·å–æ¶ˆ"));
            }

            progress.current_step = (index + 1) as u32;
            progress.current_file = Some(file_path.clone());
            progress.status = format!("åˆ†ææ–‡ä»¶ {}/{}: {}", index + 1, total_files, file_path);

            let summary_result = self.analyze_single_file_with_stream(
                file_path,
                diff_content,
                template_id,
                &session_id,
                index as u32 + 1,
                total_files as u32,
                repository_path.clone(),
                &progress_callback,
            ).await?;

            file_summaries.push(summary_result.summary.clone());
            conversation_records.push(summary_result.record_id);
            progress.file_summaries.push(summary_result.summary);
        }

        // ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæœ€ç»ˆçš„æäº¤æ¶ˆæ¯
        progress.current_step = total_files as u32 + 1;
        progress.current_file = None;
        progress.status = "ç”Ÿæˆæœ€ç»ˆæäº¤æ¶ˆæ¯".to_string();

        let final_result = self.generate_final_commit_message_with_stream(
            template_id,
            &file_summaries,
            branch_name,
            &session_id,
            repository_path.clone(),
            &progress_callback,
        ).await?;

        conversation_records.push(final_result.record_id);

        let total_time = start_time.elapsed().as_millis() as u64;

        Ok(LayeredCommitResult {
            session_id,
            final_message: final_result.message,
            file_summaries,
            total_processing_time_ms: total_time,
            conversation_records,
            reasoning_content: final_result.reasoning_content, // æ·»åŠ æ¨ç†å†…å®¹ - Author: Evilek, Date: 2025-01-10
        })
    }

    /// è·å–æ–‡ä»¶åŠå…¶diffå†…å®¹
    /// Author: Evilek, Date: 2025-01-08
    /// æ”¯æŒå¤„ç†å¸¦æœ‰ç‰¹æ®Šæ ‡è®°çš„æ–‡ä»¶ï¼ˆ#truncated, #splitï¼‰
    /// Updated: Evilek, Date: 2025-01-09 - æ·»åŠ template_idå‚æ•°ç”¨äºæ–‡ä»¶åˆ†å‰²æ§åˆ¶
    async fn get_files_with_diffs(&self, staged_files: &[String], template_id: &str) -> Result<Vec<(String, String)>> {
        let git_engine = self.git_engine.read().await;
        let mut files_with_diffs = Vec::new();

        for file_path in staged_files {
            // æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰ç‰¹æ®Šæ ‡è®°
            if file_path.contains("#truncated") {
                // æ–°å¢æ–‡ä»¶æˆªå–å¤„ç†
                let actual_path = file_path.replace("#truncated", "");
                match git_engine.get_simple_file_diff(&actual_path) {
                    Ok(diff_content) => {
                        // æˆªå–æ–‡ä»¶å†…å®¹çš„å‰é¢éƒ¨åˆ†ï¼ˆæ ¹æ®æ¨¡æ¿çš„max_tokensé™åˆ¶ï¼‰
                        let truncated_content = self.truncate_new_file_content_with_template(&actual_path, &diff_content, template_id).await?;
                        files_with_diffs.push((actual_path, truncated_content));
                    },
                    Err(e) => {
                        return Err(anyhow::anyhow!("è·å–æ–°å¢æ–‡ä»¶diffå¤±è´¥ {}: {}", actual_path, e));
                    }
                }
            } else if file_path.contains("#split") {
                // å˜æ›´æ–‡ä»¶åˆ†å‰²å¤„ç†
                let actual_path = file_path.replace("#split", "");
                match git_engine.get_simple_file_diff(&actual_path) {
                    Ok(diff_content) => {
                        // åˆ†å‰²å¤§æ–‡ä»¶å†…å®¹ï¼Œä½¿ç”¨æ¨¡æ¿çš„max_tokensä½œä¸ºåˆ†å‰²ä¾æ®
                        let split_contents = self.split_file_content_with_template(&actual_path, &diff_content, template_id).await?;
                        for (index, content) in split_contents.into_iter().enumerate() {
                            let split_path = format!("{}#part{}", actual_path, index + 1);
                            files_with_diffs.push((split_path, content));
                        }
                    },
                    Err(e) => {
                        return Err(anyhow::anyhow!("è·å–åˆ†å‰²æ–‡ä»¶diffå¤±è´¥ {}: {}", actual_path, e));
                    }
                }

            } else {
                // æ™®é€šæ–‡ä»¶å¤„ç†
                match git_engine.get_simple_file_diff(file_path) {
                    Ok(diff_content) => {
                        files_with_diffs.push((file_path.clone(), diff_content));
                    },
                    Err(e) => {
                        return Err(anyhow::anyhow!("è·å–æ–‡ä»¶diffå¤±è´¥ {}: {}", file_path, e));
                    }
                }
            }
        }

        Ok(files_with_diffs)
    }

    /// åˆ†æå•ä¸ªæ–‡ä»¶ï¼ˆå¸¦æµå¼è¾“å‡ºæ”¯æŒï¼‰
    /// Author: Evilek, Date: 2025-01-10
    async fn analyze_single_file_with_stream<F>(
        &self,
        file_path: &str,
        diff_content: &str,
        template_id: &str,
        session_id: &str,
        step_index: u32,
        total_steps: u32,
        repository_path: Option<String>,
        progress_callback: &F,
    ) -> Result<SingleFileResult>
    where
        F: Fn(LayeredCommitProgress) + Send + Sync,
    {
        let ai_manager = self.ai_manager.read().await;
        let config = ai_manager.get_config().await;

        // ä½¿ç”¨ç»Ÿä¸€çš„æ¨¡æ¿ç³»ç»Ÿç”Ÿæˆæç¤ºè¯ï¼ˆé‡æ„ä¼˜åŒ–ï¼‰
        // Author: Evilek, Date: 2025-01-08
        // ç§»é™¤æ‰¹é‡æ–‡ä»¶å¤„ç†é€»è¾‘ï¼Œæ”¹ä¸ºå•æ–‡ä»¶ç‹¬ç«‹å¤„ç† - Author: Evilek, Date: 2025-01-09
        let context = CommitContext {
            diff: diff_content.to_string(),
            staged_files: vec![file_path.to_string()],
            branch_name: None,
            commit_type: None,
            max_length: None,
            language: "zh-CN".to_string(),
        };

        // ä½¿ç”¨PromptManagerç”Ÿæˆæ¶ˆæ¯ï¼Œç»Ÿä¸€æ¨¡æ¿ç³»ç»Ÿ
        let prompt_manager = PromptManager::new();
        let messages = prompt_manager
            .generate_file_analysis_messages(template_id, file_path, diff_content, &context)
            .map_err(|e| anyhow::anyhow!("ç”Ÿæˆæ–‡ä»¶åˆ†ææ¶ˆæ¯å¤±è´¥: {}", e))?;

        // è½¬æ¢ä¸ºAIRequestæ ¼å¼ï¼Œç§»é™¤max_tokensé™åˆ¶ç¡®ä¿å®Œæ•´è¾“å‡º - Author: Evilek, Date: 2025-01-10
        let request = AIRequest {
            messages,
            model: config.base.model.clone(),
            temperature: Some(0.3),
            max_tokens: None,  // ç§»é™¤tokené™åˆ¶ï¼Œè®©AIå®Œæ•´è¾“å‡º
            stream: Some(false),
        };

        // æ˜¾ç¤ºAIåˆ†æå¼€å§‹çŠ¶æ€ - Author: Evilek, Date: 2025-01-10
        let mut progress = LayeredCommitProgress {
            session_id: session_id.to_string(),
            current_step: step_index,
            total_steps,
            current_file: Some(file_path.to_string()),
            status: format!("åˆ†ææ–‡ä»¶ {}/{}: {}", step_index, total_steps, file_path),
            file_summaries: Vec::new(),
            ai_stream_content: Some(format!("âš¡ æ­£åœ¨åˆ†ææ–‡ä»¶: {}\n\nğŸ“¤ å‘é€è¯·æ±‚åˆ°AI...", file_path)),
        };
        progress_callback(progress.clone());

        // åœ¨AIè°ƒç”¨è¿‡ç¨‹ä¸­æä¾›æµå¼æ›´æ–° - Author: Evilek, Date: 2025-01-10
        let start_time = std::time::Instant::now();

        // åˆ›å»ºä¸€ä¸ªfutureæ¥å¤„ç†AIè°ƒç”¨ï¼Œå¹¶ä½¿ç”¨pin!å®å›ºå®šå®ƒ
        let ai_future = ai_manager.generate_commit_message(request.clone());
        tokio::pin!(ai_future);

        // ä½¿ç”¨tokio::selectæ¥åŒæ—¶å¤„ç†AIè°ƒç”¨å’Œè¿›åº¦æ›´æ–°
        let response = {
            let mut interval = tokio::time::interval(tokio::time::Duration::from_millis(1000));
            let mut step = 0;

            loop {
                tokio::select! {
                    result = &mut ai_future => {
                        // AIè°ƒç”¨å®Œæˆ
                        break result?;
                    }
                    _ = interval.tick() => {
                        // æ›´æ–°è¿›åº¦
                        step += 1;
                        let content = match step {
                            1 => format!("âš¡ æ­£åœ¨åˆ†ææ–‡ä»¶: {}\n\nâ³ AIæ­£åœ¨æ¥æ”¶å’Œå¤„ç†è¯·æ±‚...", file_path),
                            2 => format!("âš¡ æ­£åœ¨åˆ†ææ–‡ä»¶: {}\n\nğŸ§  AIæ­£åœ¨åˆ†æä»£ç å˜æ›´...", file_path),
                            3 => format!("âš¡ æ­£åœ¨åˆ†ææ–‡ä»¶: {}\n\nğŸ’­ AIæ­£åœ¨ç”Ÿæˆåˆ†æç»“æœ...", file_path),
                            _ => format!("âš¡ æ­£åœ¨åˆ†ææ–‡ä»¶: {}\n\nâ³ AIæ­£åœ¨å®Œæˆåˆ†æ...", file_path),
                        };

                        progress.ai_stream_content = Some(content);
                        progress_callback(progress.clone());
                    }
                }
            }
        };

        let processing_time = start_time.elapsed().as_millis() as u64;

        // æ¨¡æ‹Ÿæµå¼æ˜¾ç¤ºAIçš„çœŸå®å“åº”å†…å®¹ - Author: Evilek, Date: 2025-01-10
        let mut ai_output = String::new();

        // å¦‚æœæœ‰æ¨ç†å†…å®¹ï¼Œå…ˆæµå¼æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
        if let Some(reasoning) = &response.reasoning_content {
            ai_output.push_str("ğŸ§  AIæ¨ç†è¿‡ç¨‹:\n<think>\n");

            // é€å­—ç¬¦æ˜¾ç¤ºæ¨ç†å†…å®¹ï¼Œå‡å°‘å»¶è¿Ÿæé«˜å“åº”é€Ÿåº¦
            let reasoning_chars: Vec<char> = reasoning.chars().collect();
            let chunk_size = 30; // å¢åŠ æ¯æ¬¡æ˜¾ç¤ºçš„å­—ç¬¦æ•°

            for chunk in reasoning_chars.chunks(chunk_size) {
                let chunk_str: String = chunk.iter().collect();
                ai_output.push_str(&chunk_str);

                progress.ai_stream_content = Some(format!("{}\n</think>\n\nğŸ“ æ­£åœ¨ç”Ÿæˆåˆ†æç»“æœ...", ai_output));
                progress_callback(progress.clone());

                // å‡å°‘å»¶è¿Ÿï¼Œæé«˜æµå¼è¾“å‡ºé€Ÿåº¦ - Author: Evilek, Date: 2025-01-10
                tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;
            }

            ai_output.push_str("\n</think>\n\n");
        }

        // æµå¼æ˜¾ç¤ºåˆ†æç»“æœ
        ai_output.push_str("ğŸ“ åˆ†æç»“æœ:\n");
        let content_chars: Vec<char> = response.content.chars().collect();
        let chunk_size = 25; // å¢åŠ æ¯æ¬¡æ˜¾ç¤ºçš„å­—ç¬¦æ•°

        for chunk in content_chars.chunks(chunk_size) {
            let chunk_str: String = chunk.iter().collect();
            ai_output.push_str(&chunk_str);

            progress.ai_stream_content = Some(ai_output.clone());
            progress_callback(progress.clone());

            // å‡å°‘å»¶è¿Ÿï¼Œæé«˜æµå¼è¾“å‡ºé€Ÿåº¦ - Author: Evilek, Date: 2025-01-10
            tokio::time::sleep(tokio::time::Duration::from_millis(40)).await;
        }

        // è®°å½•å¯¹è¯
        let step_info = StepInfo {
            step_type: "file_analysis".to_string(),
            step_index: Some(step_index),
            total_steps: Some(total_steps),
            file_path: Some(file_path.to_string()),
            description: Some(format!("åˆ†ææ–‡ä»¶: {}", file_path)),
        };

        let record_id = ai_manager.log_conversation_with_session(
            template_id.to_string(),
            repository_path,
            Some(session_id.to_string()),
            Some("layered".to_string()),
            Some(step_info),
            request,
            response.clone(),
            processing_time,
        ).await?;

        let summary = FileSummary {
            file_path: file_path.to_string(),
            summary: response.content.clone(),
            tokens_used: response.usage.map(|u| u.total_tokens).unwrap_or(0),
        };

        Ok(SingleFileResult {
            summary,
            record_id,
        })
    }

    /// åˆ†æå•ä¸ªæ–‡ä»¶ï¼ˆåŸæ–¹æ³•ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
    async fn analyze_single_file(
        &self,
        file_path: &str,
        diff_content: &str,
        template_id: &str,
        session_id: &str,
        step_index: u32,
        total_steps: u32,
        repository_path: Option<String>,
    ) -> Result<SingleFileResult> {
        let ai_manager = self.ai_manager.read().await;
        let config = ai_manager.get_config().await;

        // ä½¿ç”¨ç»Ÿä¸€çš„æ¨¡æ¿ç³»ç»Ÿç”Ÿæˆæç¤ºè¯ï¼ˆé‡æ„ä¼˜åŒ–ï¼‰
        // Author: Evilek, Date: 2025-01-08
        // ç§»é™¤æ‰¹é‡æ–‡ä»¶å¤„ç†é€»è¾‘ï¼Œæ”¹ä¸ºå•æ–‡ä»¶ç‹¬ç«‹å¤„ç† - Author: Evilek, Date: 2025-01-09
        let context = CommitContext {
            diff: diff_content.to_string(),
            staged_files: vec![file_path.to_string()],
            branch_name: None,
            commit_type: None,
            max_length: None,
            language: Self::convert_ai_language_to_code(&config.base.language),
        };

        // ä½¿ç”¨PromptManagerç”Ÿæˆæ¶ˆæ¯ï¼Œç»Ÿä¸€æ¨¡æ¿ç³»ç»Ÿ
        let prompt_manager = PromptManager::new();
        let messages = prompt_manager
            .generate_file_analysis_messages(template_id, file_path, diff_content, &context)
            .map_err(|e| anyhow::anyhow!("ç”Ÿæˆæ–‡ä»¶åˆ†ææ¶ˆæ¯å¤±è´¥: {}", e))?;

        // è½¬æ¢ä¸ºAIRequestæ ¼å¼ - Author: Evilek, Date: 2025-01-10
        let request = AIRequest {
            messages,
            model: config.base.model.clone(),
            temperature: Some(0.3),
            max_tokens: Some(config.advanced.max_tokens), // ä½¿ç”¨ç³»ç»Ÿå…¨å±€é…ç½®çš„max_tokensï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 
            stream: Some(false),  // å½“å‰æä¾›å•†å®ç°ä¸æ”¯æŒæµå¼è¾“å‡ºï¼Œä¿æŒfalse
        };

        let start_time = std::time::Instant::now();
        let response = ai_manager.generate_commit_message(request.clone()).await?;
        let processing_time = start_time.elapsed().as_millis() as u64;

        // è®°å½•å¯¹è¯åˆ°æ—¥å¿—
        let step_info = StepInfo {
            step_type: "file_analysis".to_string(),
            step_index: Some(step_index),
            total_steps: Some(total_steps),
            file_path: Some(file_path.to_string()),
            description: Some(format!("åˆ†ææ–‡ä»¶: {}", file_path)),
        };

        // è®°å½•å¯¹è¯åˆ°æ—¥å¿—
        let record_id = ai_manager.log_conversation_with_session(
            "layered_commit".to_string(),
            repository_path,
            Some(session_id.to_string()),
            Some("layered".to_string()),
            Some(step_info),
            request,
            response.clone(),
            processing_time,
        ).await?;

        let tokens_used = TokenCounter::estimate_tokens(&response.content);

        Ok(SingleFileResult {
            summary: FileSummary {
                file_path: file_path.to_string(),
                summary: response.content,
                tokens_used,
            },
            record_id,
        })
    }

    /// ç”Ÿæˆæœ€ç»ˆæäº¤æ¶ˆæ¯ï¼ˆå¸¦æµå¼è¾“å‡ºæ”¯æŒï¼‰
    /// Author: Evilek, Date: 2025-01-10
    async fn generate_final_commit_message_with_stream<F>(
        &self,
        template_id: &str,
        file_summaries: &[FileSummary],
        _branch_name: Option<String>,
        session_id: &str,
        repository_path: Option<String>,
        progress_callback: &F,
    ) -> Result<FinalCommitResult>
    where
        F: Fn(LayeredCommitProgress) + Send + Sync,
    {
        let ai_manager = self.ai_manager.read().await;

        // æ„å»ºæ±‡æ€»çš„diffå†…å®¹
        let summary_content = file_summaries
            .iter()
            .map(|fs| format!("æ–‡ä»¶: {}\næ‘˜è¦: {}", fs.file_path, fs.summary))
            .collect::<Vec<_>>()
            .join("\n\n");

        // è·å–æ¨¡æ¿å†…å®¹å¹¶æ„å»ºæœ€ç»ˆæ€»ç»“çš„æç¤ºè¯
        let prompt_manager = ai_manager.get_prompt_manager().await;
        let _template = prompt_manager.get_template(template_id)
            .ok_or_else(|| anyhow::anyhow!("Template '{}' not found", template_id))?;

        // æ„å»ºä¸Šä¸‹æ–‡
        let context = CommitContext {
            diff: summary_content.clone(),
            staged_files: file_summaries.iter().map(|fs| fs.file_path.clone()).collect(),
            branch_name: None,
            commit_type: None,
            max_length: None,
            language: "zh-CN".to_string(),
        };

        // ä½¿ç”¨ç»Ÿä¸€ç”Ÿæˆçš„æ¶ˆæ¯ï¼ˆé‡æ„ä¼˜åŒ–ï¼‰
        let file_summary_strs: Vec<&str> = file_summaries.iter().map(|fs| fs.summary.as_str()).collect();
        let messages = prompt_manager.generate_summary_messages(template_id, &context, &file_summary_strs)
            .map_err(|e| anyhow::anyhow!("ç”Ÿæˆæ€»ç»“æ¶ˆæ¯å¤±è´¥: {}", e))?;

        // ä½¿ç”¨ç»Ÿä¸€ç”Ÿæˆçš„æ¶ˆæ¯ï¼ˆé‡æ„ä¼˜åŒ–ï¼‰ï¼Œç§»é™¤max_tokensé™åˆ¶ - Author: Evilek, Date: 2025-01-10
        let config = ai_manager.get_config().await;
        let request = AIRequest {
            messages: messages.clone(),
            model: config.base.model.clone(),
            temperature: Some(0.3),
            max_tokens: None,  // ç§»é™¤tokené™åˆ¶ï¼Œè®©AIå®Œæ•´è¾“å‡ºæœ€ç»ˆæäº¤æ¶ˆæ¯
            stream: Some(false),
        };

        // æ˜¾ç¤ºæœ€ç»ˆæäº¤æ¶ˆæ¯ç”Ÿæˆå¼€å§‹çŠ¶æ€ - Author: Evilek, Date: 2025-01-10
        let mut progress = LayeredCommitProgress {
            session_id: session_id.to_string(),
            current_step: file_summaries.len() as u32 + 1,
            total_steps: file_summaries.len() as u32 + 1,
            current_file: None,
            status: "ç”Ÿæˆæœ€ç»ˆæäº¤æ¶ˆæ¯".to_string(),
            file_summaries: file_summaries.to_vec(),  // ä¿æŒå·²æœ‰çš„æ–‡ä»¶æ‘˜è¦ - Author: Evilek, Date: 2025-01-10
            ai_stream_content: Some("ğŸ¯ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæäº¤æ¶ˆæ¯...\n\nğŸ“¤ å‘é€æ±‡æ€»è¯·æ±‚åˆ°AI...".to_string()),
        };
        progress_callback(progress.clone());

        // åœ¨AIè°ƒç”¨è¿‡ç¨‹ä¸­æä¾›æµå¼æ›´æ–° - Author: Evilek, Date: 2025-01-10
        let start_time = std::time::Instant::now();
        let file_count = file_summaries.len();

        // åˆ›å»ºä¸€ä¸ªfutureæ¥å¤„ç†AIè°ƒç”¨ï¼Œå¹¶ä½¿ç”¨pin!å®å›ºå®šå®ƒ
        let ai_future = ai_manager.generate_commit_message(request.clone());
        tokio::pin!(ai_future);

        // ä½¿ç”¨tokio::selectæ¥åŒæ—¶å¤„ç†AIè°ƒç”¨å’Œè¿›åº¦æ›´æ–°
        let response = {
            let mut interval = tokio::time::interval(tokio::time::Duration::from_millis(1200));
            let mut step = 0;

            loop {
                tokio::select! {
                    result = &mut ai_future => {
                        // AIè°ƒç”¨å®Œæˆ
                        break result?;
                    }
                    _ = interval.tick() => {
                        // æ›´æ–°è¿›åº¦
                        step += 1;
                        let content = match step {
                            1 => format!("ğŸ¯ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæäº¤æ¶ˆæ¯...\n\nğŸ“Š åŸºäº {} ä¸ªæ–‡ä»¶çš„åˆ†æç»“æœ\nâ³ AIæ­£åœ¨æ¥æ”¶å’Œå¤„ç†æ±‡æ€»è¯·æ±‚...", file_count),
                            2 => format!("ğŸ¯ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæäº¤æ¶ˆæ¯...\n\nğŸ“Š åŸºäº {} ä¸ªæ–‡ä»¶çš„åˆ†æç»“æœ\nğŸ§  AIæ­£åœ¨æ•´åˆæ‰€æœ‰åˆ†æç»“æœ...", file_count),
                            3 => format!("ğŸ¯ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæäº¤æ¶ˆæ¯...\n\nğŸ“Š åŸºäº {} ä¸ªæ–‡ä»¶çš„åˆ†æç»“æœ\nğŸ¨ AIæ­£åœ¨ç”Ÿæˆç»Ÿä¸€çš„æäº¤æ¶ˆæ¯...", file_count),
                            _ => format!("ğŸ¯ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæäº¤æ¶ˆæ¯...\n\nğŸ“Š åŸºäº {} ä¸ªæ–‡ä»¶çš„åˆ†æç»“æœ\nâ³ AIæ­£åœ¨å®Œæˆæäº¤æ¶ˆæ¯ç”Ÿæˆ...", file_count),
                        };

                        progress.ai_stream_content = Some(content);
                        progress_callback(progress.clone());
                    }
                }
            }
        };

        let processing_time = start_time.elapsed().as_millis() as u64;

        // æ¨¡æ‹Ÿæµå¼æ˜¾ç¤ºAIçš„çœŸå®å“åº”å†…å®¹ - Author: Evilek, Date: 2025-01-10
        let mut ai_output = String::new();

        // å¦‚æœæœ‰æ¨ç†å†…å®¹ï¼Œå…ˆæµå¼æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
        if let Some(reasoning) = &response.reasoning_content {
            ai_output.push_str("ğŸ§  AIæ¨ç†è¿‡ç¨‹:\n<think>\n");

            // é€å­—ç¬¦æ˜¾ç¤ºæ¨ç†å†…å®¹ï¼Œä¼˜åŒ–é€Ÿåº¦
            let reasoning_chars: Vec<char> = reasoning.chars().collect();
            let chunk_size = 35; // å¢åŠ æ¯æ¬¡æ˜¾ç¤ºçš„å­—ç¬¦æ•°

            for chunk in reasoning_chars.chunks(chunk_size) {
                let chunk_str: String = chunk.iter().collect();
                ai_output.push_str(&chunk_str);

                progress.ai_stream_content = Some(format!("{}\n</think>\n\nğŸ“ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæäº¤æ¶ˆæ¯...", ai_output));
                progress_callback(progress.clone());

                // å‡å°‘å»¶è¿Ÿï¼Œæé«˜æµå¼è¾“å‡ºé€Ÿåº¦ - Author: Evilek, Date: 2025-01-10
                tokio::time::sleep(tokio::time::Duration::from_millis(60)).await;
            }

            ai_output.push_str("\n</think>\n\n");
        }

        // æµå¼æ˜¾ç¤ºæœ€ç»ˆæäº¤æ¶ˆæ¯
        ai_output.push_str("ğŸ“ æœ€ç»ˆæäº¤æ¶ˆæ¯:\n");
        let content_chars: Vec<char> = response.content.chars().collect();
        let chunk_size = 30; // å¢åŠ æ¯æ¬¡æ˜¾ç¤ºçš„å­—ç¬¦æ•°

        for chunk in content_chars.chunks(chunk_size) {
            let chunk_str: String = chunk.iter().collect();
            ai_output.push_str(&chunk_str);

            progress.ai_stream_content = Some(ai_output.clone());
            progress_callback(progress.clone());

            // å‡å°‘å»¶è¿Ÿï¼Œæé«˜æµå¼è¾“å‡ºé€Ÿåº¦ - Author: Evilek, Date: 2025-01-10
            tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;
        }

        // è®°å½•æœ€ç»ˆæäº¤æ¶ˆæ¯ç”Ÿæˆçš„å¯¹è¯
        let step_info = StepInfo {
            step_type: "final_commit_generation".to_string(),
            step_index: None,
            total_steps: None,
            file_path: None,
            description: Some("ç”Ÿæˆæœ€ç»ˆæäº¤æ¶ˆæ¯".to_string()),
        };

        let record_id = ai_manager.log_conversation_with_session(
            template_id.to_string(),
            repository_path,
            Some(session_id.to_string()),
            Some("layered".to_string()),
            Some(step_info),
            request,
            response.clone(),
            processing_time,
        ).await?;

        Ok(FinalCommitResult {
            message: response.content,
            record_id,
            reasoning_content: response.reasoning_content,
        })
    }

    /// ç”Ÿæˆæœ€ç»ˆæäº¤æ¶ˆæ¯ï¼ˆåŸæ–¹æ³•ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
    /// ä½œè€…ï¼šEvilek
    /// ç¼–å†™æ—¥æœŸï¼š2025-08-05
    async fn generate_final_commit_message(
        &self,
        template_id: &str,
        file_summaries: &[FileSummary],
        _branch_name: Option<String>,
        session_id: &str,
        repository_path: Option<String>,
    ) -> Result<FinalCommitResult> {
        let ai_manager = self.ai_manager.read().await;

        // æ„å»ºæ±‡æ€»çš„diffå†…å®¹
        let summary_content = file_summaries
            .iter()
            .map(|fs| format!("æ–‡ä»¶: {}\næ‘˜è¦: {}", fs.file_path, fs.summary))
            .collect::<Vec<_>>()
            .join("\n\n");

        // è·å–æ¨¡æ¿å†…å®¹å¹¶æ„å»ºæœ€ç»ˆæ€»ç»“çš„æç¤ºè¯
        let prompt_manager = ai_manager.get_prompt_manager().await;
        let _template = prompt_manager.get_template(template_id)
            .ok_or_else(|| anyhow::anyhow!("Template '{}' not found", template_id))?;

        // æ„å»ºä¸´æ—¶çš„CommitContextç”¨äºè¯­è¨€å¤„ç†
        let temp_context = crate::core::prompt_manager::CommitContext {
            diff: summary_content.clone(),
            staged_files: file_summaries.iter().map(|fs| fs.file_path.clone()).collect(),
            branch_name: _branch_name.clone(),
            commit_type: None,
            max_length: None,
            language: {
                // ä»AIé…ç½®ä¸­è·å–è¯­è¨€è®¾ç½®
                let config = ai_manager.get_config().await;
                Self::convert_ai_language_to_code(&config.base.language)
            },
        };

        // ä½¿ç”¨ç»Ÿä¸€çš„æ€»ç»“æ¶ˆæ¯ç”Ÿæˆç³»ç»Ÿï¼ˆé‡æ„ä¼˜åŒ–ï¼‰
        // Author: Evilek, Date: 2025-01-08
        let file_summaries: Vec<&str> = file_summaries.iter().map(|s| s.summary.as_str()).collect();
        let messages = prompt_manager
            .generate_summary_messages(template_id, &temp_context, &file_summaries)
            .map_err(|e| anyhow::anyhow!("ç”Ÿæˆæ€»ç»“æ¶ˆæ¯å¤±è´¥: {}", e))?;

        // ä½¿ç”¨ç»Ÿä¸€ç”Ÿæˆçš„æ¶ˆæ¯ï¼ˆé‡æ„ä¼˜åŒ–ï¼‰
        let config = ai_manager.get_config().await;
        let request = AIRequest {
            messages,
            model: config.base.model.clone(),
            temperature: Some(0.3),
            max_tokens: Some(config.advanced.max_tokens), // ä½¿ç”¨ç³»ç»Ÿå…¨å±€é…ç½®çš„max_tokensï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 
            stream: Some(false),
        };

        let start_time = std::time::Instant::now();
        let response = ai_manager.generate_commit_message(request.clone()).await?;
        let processing_time = start_time.elapsed().as_millis() as u64;

        // è®°å½•æœ€ç»ˆæäº¤æ¶ˆæ¯ç”Ÿæˆçš„å¯¹è¯
        let step_info = StepInfo {
            step_type: "final_commit_generation".to_string(),
            step_index: None,
            total_steps: None,
            file_path: None,
            description: Some("ç”Ÿæˆæœ€ç»ˆæäº¤æ¶ˆæ¯".to_string()),
        };

        let record_id = ai_manager.log_conversation_with_session(
            "layered_commit".to_string(),
            repository_path,
            Some(session_id.to_string()),
            Some("layered".to_string()),
            Some(step_info),
            request,
            response.clone(),
            processing_time,
        ).await?;

        Ok(FinalCommitResult {
            message: response.content,
            record_id,
            reasoning_content: response.reasoning_content, // æ·»åŠ æ¨ç†å†…å®¹ - Author: Evilek, Date: 2025-01-10
        })
    }

    /// è·å–æ¨¡å‹çš„æœ€å¤§tokené™åˆ¶
    async fn get_model_max_tokens(&self, model_id: &str) -> Result<Option<u32>> {
        // ç®€åŒ–å®ç°ï¼Œè¿”å›å¸¸è§æ¨¡å‹çš„tokené™åˆ¶
        // Author: Evilek, Date: 2025-01-09 - æ·»åŠ qwenæ¨¡å‹æ”¯æŒ
        let max_tokens = match model_id {
            m if m.contains("gpt-4") => Some(8192),
            m if m.contains("gpt-3.5") => Some(4096),
            m if m.contains("claude") => Some(100000),
            m if m.contains("gemini") => Some(32768),
            m if m.contains("qwen2.5:32b") => Some(32768), // qwen2.5:32b æ”¯æŒ32kä¸Šä¸‹æ–‡
            m if m.contains("qwen") => Some(8192), // å…¶ä»–qwenæ¨¡å‹é»˜è®¤8k
            _ => Some(4096), // é»˜è®¤é™åˆ¶
        };

        Ok(max_tokens)
    }

    /// æˆªå–æ–°å¢æ–‡ä»¶å†…å®¹ï¼ˆä½¿ç”¨æ¨¡æ¿é…ç½®ï¼‰
    /// Author: Evilek, Date: 2025-01-09
    /// æ ¹æ®æ¨¡æ¿çš„max_tokensé™åˆ¶æˆªå–æ–°å¢æ–‡ä»¶çš„å‰é¢éƒ¨åˆ†ï¼Œå¹¶åŒ…å«æ–‡ä»¶åä¸Šä¸‹æ–‡
    async fn truncate_new_file_content_with_template(&self, file_path: &str, diff_content: &str, template_id: &str) -> Result<String> {
        // è·å–æ¨¡æ¿çš„max_tokensé…ç½®ä½œä¸ºæˆªå–ä¾æ®
        // Author: Evilek, Date: 2025-01-09 - ä¿®å¤PromptManagerå®ä¾‹åŒ–é—®é¢˜ï¼Œä½¿ç”¨AIç®¡ç†å™¨ä¸­çš„å®ä¾‹
        let ai_manager = self.ai_manager.read().await;
        let prompt_manager = ai_manager.get_prompt_manager().await;
        let template_max_tokens = prompt_manager.get_template_config(template_id)
            .and_then(|(max_tokens, _)| max_tokens)
            .unwrap_or(1000); // ä¿®å¤ï¼šå¢åŠ é»˜è®¤å€¼åˆ°1000 tokensï¼Œé¿å…è¿‡åº¦æˆªå–

        // ä½¿ç”¨æ¨¡æ¿çš„max_tokensä½œä¸ºæˆªå–çš„å®‰å…¨é™åˆ¶ï¼ˆä¿ç•™30%ä½™é‡ç»™æ–‡ä»¶åå’Œæ ¼å¼ï¼‰
        let safe_limit = (template_max_tokens as f32 * 0.7) as u32;

        // é¢„ä¼°æ–‡ä»¶åå’Œæ ¼å¼å¼€é”€çš„tokenæ•°
        let file_context_tokens = TokenCounter::estimate_tokens(&format!("æ–‡ä»¶: {}\n\n", file_path)) + 50;

        let lines: Vec<&str> = diff_content.lines().collect();
        let total_lines = lines.len();
        let mut truncated_lines = Vec::new();
        let mut current_tokens = file_context_tokens;

        for line in &lines {
            let line_tokens = TokenCounter::estimate_tokens(line);
            if current_tokens + line_tokens > safe_limit {
                break;
            }
            truncated_lines.push(*line);
            current_tokens += line_tokens;
        }

        let truncated_content = truncated_lines.join("\n");
        let truncated_line_count = truncated_lines.len();

        // æ·»åŠ æ–‡ä»¶åä¸Šä¸‹æ–‡å’Œæˆªå–è¯´æ˜
        let result = if truncated_line_count < total_lines {
            format!("æ–‡ä»¶: {}\n\n{}\n\n# æ–‡ä»¶å†…å®¹å·²æˆªå–ï¼Œæ˜¾ç¤ºå‰{}è¡Œï¼ˆå…±{}è¡Œï¼‰",
                file_path, truncated_content, truncated_line_count, total_lines)
        } else {
            format!("æ–‡ä»¶: {}\n\n{}", file_path, truncated_content)
        };

        Ok(result)
    }

    /// æˆªå–æ–°å¢æ–‡ä»¶å†…å®¹ï¼ˆä¿ç•™åŸæ–¹æ³•ç”¨äºå‘åå…¼å®¹ï¼‰
    /// Author: Evilek, Date: 2025-01-08
    /// æ ¹æ®tokené™åˆ¶æˆªå–æ–°å¢æ–‡ä»¶çš„å‰é¢éƒ¨åˆ†
    async fn truncate_new_file_content(&self, diff_content: &str) -> Result<String> {
        let ai_manager = self.ai_manager.read().await;
        let config = ai_manager.get_config().await;
        let model_max_tokens = self.get_model_max_tokens(&config.base.model).await?;

        // è®¡ç®—å®‰å…¨çš„tokené™åˆ¶ï¼ˆä¿ç•™30%ä½™é‡ï¼‰
        let safe_limit = if let Some(max_tokens) = model_max_tokens {
            (max_tokens as f32 * 0.7) as u32
        } else {
            2800 // é»˜è®¤å®‰å…¨é™åˆ¶
        };

        let lines: Vec<&str> = diff_content.lines().collect();
        let total_lines = lines.len();
        let mut truncated_lines = Vec::new();
        let mut current_tokens = 0u32;

        for line in &lines {
            let line_tokens = TokenCounter::estimate_tokens(line);
            if current_tokens + line_tokens > safe_limit {
                break;
            }
            truncated_lines.push(*line);
            current_tokens += line_tokens;
        }

        // æ·»åŠ æˆªå–è¯´æ˜
        let mut result = truncated_lines.join("\n");
        if truncated_lines.len() < total_lines {
            result.push_str("\n...");
            result.push_str(&format!("\n# æ–‡ä»¶å†…å®¹å·²æˆªå–ï¼Œæ˜¾ç¤ºå‰{}è¡Œï¼ˆå…±{}è¡Œï¼‰", truncated_lines.len(), total_lines));
        }

        Ok(result)
    }

    /// åˆ†å‰²æ–‡ä»¶å†…å®¹ï¼ˆä½¿ç”¨æ¨¡æ¿é…ç½®ï¼‰
    /// Author: Evilek, Date: 2025-01-09
    /// æ ¹æ®æ¨¡æ¿çš„max_tokensé…ç½®å°†å¤§æ–‡ä»¶å†…å®¹åˆ†å‰²æˆå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½åŒ…å«æ–‡ä»¶åä¸Šä¸‹æ–‡
    async fn split_file_content_with_template(&self, file_path: &str, diff_content: &str, template_id: &str) -> Result<Vec<String>> {
        // è·å–æ¨¡æ¿çš„max_tokensé…ç½®ä½œä¸ºåˆ†å‰²ä¾æ®
        // Author: Evilek, Date: 2025-01-09 - ä¿®å¤PromptManagerå®ä¾‹åŒ–é—®é¢˜ï¼Œä½¿ç”¨AIç®¡ç†å™¨ä¸­çš„å®ä¾‹
        let ai_manager = self.ai_manager.read().await;
        let prompt_manager = ai_manager.get_prompt_manager().await;
        let template_max_tokens = prompt_manager.get_template_config(template_id)
            .and_then(|(max_tokens, _)| max_tokens)
            .unwrap_or(1000); // ä¿®å¤ï¼šå¢åŠ é»˜è®¤å€¼åˆ°1000 tokensï¼Œé¿å…è¿‡åº¦åˆ†å‰²

        println!("ğŸ” [split_file_content_with_template] æ¨¡æ¿ {} çš„max_tokens: {}", template_id, template_max_tokens);

        // ä½¿ç”¨æ¨¡æ¿çš„max_tokensä½œä¸ºåˆ†å‰²çš„å®‰å…¨é™åˆ¶ï¼ˆä¿ç•™30%ä½™é‡ç»™æ–‡ä»¶åå’Œæ ¼å¼ï¼‰
        let safe_limit = (template_max_tokens as f32 * 0.7) as u32;

        println!("ğŸ” [split_file_content_with_template] åˆ†å‰²å®‰å…¨é™åˆ¶: {} tokens", safe_limit);

        let lines: Vec<&str> = diff_content.lines().collect();
        let mut split_contents = Vec::new();
        let mut current_chunk = Vec::new();
        let mut current_tokens = 0u32;

        // é¢„ä¼°æ–‡ä»¶åå’Œæ ¼å¼å¼€é”€çš„tokenæ•°
        let file_context_tokens = TokenCounter::estimate_tokens(&format!("æ–‡ä»¶: {}\n\n", file_path)) + 50;

        for line in &lines {
            let line_tokens = TokenCounter::estimate_tokens(line);

            // å¦‚æœæ·»åŠ è¿™ä¸€è¡Œä¼šè¶…è¿‡é™åˆ¶ï¼Œä¿å­˜å½“å‰å—å¹¶å¼€å§‹æ–°å—
            if current_tokens + line_tokens + file_context_tokens > safe_limit && !current_chunk.is_empty() {
                // ä¸ºæ¯ä¸ªåˆ†å‰²éƒ¨åˆ†æ·»åŠ æ–‡ä»¶åä¸Šä¸‹æ–‡
                let chunk_with_context = format!("æ–‡ä»¶: {}\n\n{}", file_path, current_chunk.join("\n"));
                split_contents.push(chunk_with_context);
                current_chunk.clear();
                current_tokens = 0;
            }

            current_chunk.push(*line);
            current_tokens += line_tokens;
        }

        // æ·»åŠ æœ€åä¸€ä¸ªå—
        if !current_chunk.is_empty() {
            let chunk_with_context = format!("æ–‡ä»¶: {}\n\n{}", file_path, current_chunk.join("\n"));
            split_contents.push(chunk_with_context);
        }

        // å¦‚æœæœ‰å¤šä¸ªåˆ†ç‰‡ï¼Œä¸ºæ¯ä¸ªåˆ†ç‰‡æ·»åŠ åˆ†ç‰‡è¯´æ˜
        if split_contents.len() > 1 {
            let total_parts = split_contents.len();
            for (index, content) in split_contents.iter_mut().enumerate() {
                content.push_str(&format!("\n\n# è¿™æ˜¯æ–‡ä»¶ {} çš„ç¬¬{}éƒ¨åˆ†ï¼ˆå…±{}éƒ¨åˆ†ï¼‰", file_path, index + 1, total_parts));
            }
        }

        Ok(split_contents)
    }

    /// åˆ†å‰²æ–‡ä»¶å†…å®¹ï¼ˆä¿ç•™åŸæ–¹æ³•ç”¨äºå‘åå…¼å®¹ï¼‰
    /// Author: Evilek, Date: 2025-01-08
    /// å°†å¤§æ–‡ä»¶å†…å®¹åˆ†å‰²æˆå¤šä¸ªéƒ¨åˆ†
    async fn split_file_content(&self, diff_content: &str) -> Result<Vec<String>> {
        let ai_manager = self.ai_manager.read().await;
        let config = ai_manager.get_config().await;
        let model_max_tokens = self.get_model_max_tokens(&config.base.model).await?;

        // è®¡ç®—æ¯ä¸ªåˆ†ç‰‡çš„å®‰å…¨tokené™åˆ¶
        let safe_limit = if let Some(max_tokens) = model_max_tokens {
            (max_tokens as f32 * 0.6) as u32 // æ›´ä¿å®ˆçš„é™åˆ¶
        } else {
            2400 // é»˜è®¤å®‰å…¨é™åˆ¶
        };

        let lines: Vec<&str> = diff_content.lines().collect();
        let mut split_contents = Vec::new();
        let mut current_chunk = Vec::new();
        let mut current_tokens = 0u32;

        for line in &lines {
            let line_tokens = TokenCounter::estimate_tokens(line);

            // å¦‚æœæ·»åŠ è¿™ä¸€è¡Œä¼šè¶…è¿‡é™åˆ¶ï¼Œä¿å­˜å½“å‰å—å¹¶å¼€å§‹æ–°å—
            if current_tokens + line_tokens > safe_limit && !current_chunk.is_empty() {
                split_contents.push(current_chunk.join("\n"));
                current_chunk.clear();
                current_tokens = 0;
            }

            current_chunk.push(*line);
            current_tokens += line_tokens;
        }

        // æ·»åŠ æœ€åä¸€ä¸ªå—
        if !current_chunk.is_empty() {
            split_contents.push(current_chunk.join("\n"));
        }

        // å¦‚æœåªæœ‰ä¸€ä¸ªå—ï¼Œè¯´æ˜ä¸éœ€è¦åˆ†å‰²
        if split_contents.len() == 1 {
            return Ok(split_contents);
        }

        // ä¸ºæ¯ä¸ªåˆ†ç‰‡æ·»åŠ è¯´æ˜
        let total_parts = split_contents.len();
        for (index, content) in split_contents.iter_mut().enumerate() {
            content.push_str(&format!("\n# è¿™æ˜¯æ–‡ä»¶çš„ç¬¬{}éƒ¨åˆ†ï¼ˆå…±{}éƒ¨åˆ†ï¼‰", index + 1, total_parts));
        }

        Ok(split_contents)
    }

    /// å°†AIé…ç½®ä¸­çš„è¯­è¨€åç§°è½¬æ¢ä¸ºè¯­è¨€ä»£ç 
    /// Author: Evilek, Date: 2025-01-08
    /// ç»Ÿä¸€è¯­è¨€è½¬æ¢é€»è¾‘ï¼Œé¿å…ä»£ç é‡å¤
    fn convert_ai_language_to_code(language_name: &str) -> String {
        match language_name {
            "Simplified Chinese" => "zh-CN",
            "Traditional Chinese" => "zh-TW",
            "English" => "en",
            "Japanese" => "ja",
            "Korean" => "ko",
            "French" => "fr",
            "German" => "de",
            "Spanish" => "es",
            "Russian" => "ru",
            "Portuguese" => "pt",
            "Italian" => "it",
            "Dutch" => "nl",
            "Swedish" => "sv",
            "Czech" => "cs",
            "Polish" => "pl",
            "Turkish" => "tr",
            "Vietnamese" => "vi",
            "Thai" => "th",
            "Indonesian" => "id",
            _ => "en", // é»˜è®¤è‹±æ–‡
        }.to_string()
    }
}



#[derive(Debug)]
struct SingleFileResult {
    summary: FileSummary,
    record_id: String,
}

#[derive(Debug)]
struct FinalCommitResult {
    message: String,
    record_id: String,
    /// æ¨ç†å†…å®¹ï¼ˆ<think>æ ‡ç­¾å†…çš„å†…å®¹ï¼‰
    /// ä½œè€…ï¼šEvilek
    /// ç¼–å†™æ—¥æœŸï¼š2025-01-10
    reasoning_content: Option<String>,
}
