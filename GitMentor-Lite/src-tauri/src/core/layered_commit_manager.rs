use anyhow::Result;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use tokio::sync::RwLock;
use uuid::Uuid;

use crate::core::ai_manager::AIManager;
use crate::core::git_engine::GitEngine;

use crate::core::ai_provider::{AIRequest, ChatMessage};
use crate::utils::token_counter::TokenCounter;

/**
 * åˆ†å±‚æäº¤ç®¡ç†å™¨
 * ä½œè€…ï¼šEvilek
 * ç¼–å†™æ—¥æœŸï¼š2025-08-04
 */

#[derive(Debug, Clone)]
pub struct LayeredCommitProgress {
    pub session_id: String,
    pub current_step: u32,
    pub total_steps: u32,
    pub current_file: Option<String>,
    pub status: String,
    pub file_summaries: Vec<FileSummary>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FileSummary {
    pub file_path: String,
    pub summary: String,
    pub tokens_used: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LayeredCommitResult {
    pub session_id: String,
    pub final_message: String,
    pub file_summaries: Vec<FileSummary>,
    pub total_processing_time_ms: u64,
    pub conversation_records: Vec<String>, // è®°å½•IDåˆ—è¡¨
}

pub struct LayeredCommitManager {
    ai_manager: Arc<RwLock<AIManager>>,
    git_engine: Arc<RwLock<GitEngine>>,
}

impl LayeredCommitManager {
    pub fn new(
        ai_manager: Arc<RwLock<AIManager>>,
        git_engine: Arc<RwLock<GitEngine>>,
    ) -> Self {
        Self {
            ai_manager,
            git_engine,
        }
    }

    /// æ£€æŸ¥æ˜¯å¦éœ€è¦å¯ç”¨åˆ†å±‚æäº¤
    pub async fn should_use_layered_commit(
        &self,
        template_id: &str,
        diff_content: &str,
        staged_files: &[String],
    ) -> Result<bool> {
        let ai_manager = self.ai_manager.read().await;
        let config = ai_manager.get_config().await;
        
        // æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†åˆ†å±‚æäº¤åŠŸèƒ½
        if !config.features.enable_layered_commit {
            return Ok(false);
        }

        // è·å–å½“å‰æ¨¡å‹çš„tokené™åˆ¶
        let model_max_tokens = self.get_model_max_tokens(&config.base.model).await?;
        
        // æ„å»ºåŸºæœ¬çš„æ¶ˆæ¯æ¥ä¼°ç®—tokenæ•°é‡
        let system_message = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Gitæäº¤æ¶ˆæ¯ç”ŸæˆåŠ©æ‰‹ã€‚";
        let user_message = format!("è¯·ä¸ºä»¥ä¸‹ä»£ç å˜æ›´ç”Ÿæˆæäº¤æ¶ˆæ¯:\n{}", diff_content);
        
        let messages = vec![
            ChatMessage {
                role: "system".to_string(),
                content: system_message.to_string(),
            },
            ChatMessage {
                role: "user".to_string(),
                content: user_message,
            },
        ];
        
        let request = AIRequest {
            messages,
            model: config.base.model.clone(),
            temperature: Some(0.3),
            max_tokens: Some(500),
            stream: Some(false),
        };

        let estimated_tokens = TokenCounter::estimate_request_tokens(&request);
        let is_over_limit = TokenCounter::is_over_limit(estimated_tokens, model_max_tokens);

        println!("ğŸ” [LayeredCommit] Tokenä¼°ç®—: {} / {:?}, è¶…é™: {}", 
                estimated_tokens, model_max_tokens, is_over_limit);

        Ok(is_over_limit)
    }

    /// æ‰§è¡Œåˆ†å±‚æäº¤
    pub async fn execute_layered_commit<F>(
        &self,
        template_id: &str,
        staged_files: Vec<String>,
        branch_name: Option<String>,
        repository_path: Option<String>,
        progress_callback: F,
    ) -> Result<LayeredCommitResult>
    where
        F: Fn(LayeredCommitProgress) + Send + Sync,
    {
        let session_id = Uuid::new_v4().to_string();
        let start_time = std::time::Instant::now();
        
        println!("ğŸš€ [LayeredCommit] å¼€å§‹åˆ†å±‚æäº¤ï¼Œä¼šè¯ID: {}", session_id);

        // ç¬¬ä¸€æ­¥ï¼šè·å–æ¯ä¸ªæ–‡ä»¶çš„diff
        let files_with_diffs = self.get_files_with_diffs(&staged_files).await?;
        let total_files = files_with_diffs.len();

        // åˆå§‹åŒ–è¿›åº¦
        let mut progress = LayeredCommitProgress {
            session_id: session_id.clone(),
            current_step: 0,
            total_steps: (total_files + 1) as u32, // æ–‡ä»¶åˆ†æ + æœ€ç»ˆæ€»ç»“
            current_file: None,
            status: "å¼€å§‹åˆ†å±‚æäº¤".to_string(),
            file_summaries: Vec::new(),
        };
        progress_callback(progress.clone());

        // ç¬¬äºŒæ­¥ï¼šä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆæ‘˜è¦
        let mut file_summaries = Vec::new();
        let mut conversation_records = Vec::new();

        for (index, (file_path, diff_content)) in files_with_diffs.iter().enumerate() {
            progress.current_step = (index + 1) as u32;
            progress.current_file = Some(file_path.clone());
            progress.status = format!("åˆ†ææ–‡ä»¶ {}/{}: {}", index + 1, total_files, file_path);
            progress_callback(progress.clone());

            let summary_result = self.analyze_single_file(
                file_path,
                diff_content,
                &session_id,
                index as u32 + 1,
                total_files as u32,
                repository_path.clone(),
            ).await?;

            file_summaries.push(summary_result.summary.clone());
            conversation_records.push(summary_result.record_id);
            progress.file_summaries.push(summary_result.summary);
        }

        // ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæœ€ç»ˆçš„æäº¤æ¶ˆæ¯
        progress.current_step = total_files as u32 + 1;
        progress.current_file = None;
        progress.status = "ç”Ÿæˆæœ€ç»ˆæäº¤æ¶ˆæ¯".to_string();
        progress_callback(progress.clone());

        let final_result = self.generate_final_commit_message(
            template_id,
            &file_summaries,
            branch_name,
            &session_id,
            repository_path.clone(),
        ).await?;

        conversation_records.push(final_result.record_id);

        let total_time = start_time.elapsed().as_millis() as u64;

        println!("âœ… [LayeredCommit] åˆ†å±‚æäº¤å®Œæˆï¼Œè€—æ—¶: {}ms", total_time);

        Ok(LayeredCommitResult {
            session_id,
            final_message: final_result.message,
            file_summaries,
            total_processing_time_ms: total_time,
            conversation_records,
        })
    }

    /// è·å–æ–‡ä»¶åŠå…¶diffå†…å®¹
    async fn get_files_with_diffs(&self, staged_files: &[String]) -> Result<Vec<(String, String)>> {
        let git_engine = self.git_engine.read().await;
        let mut files_with_diffs = Vec::new();

        for file_path in staged_files {
            let diff_content = git_engine.get_simple_file_diff(file_path)?;
            files_with_diffs.push((file_path.clone(), diff_content));
        }

        Ok(files_with_diffs)
    }

    /// åˆ†æå•ä¸ªæ–‡ä»¶
    async fn analyze_single_file(
        &self,
        file_path: &str,
        diff_content: &str,
        session_id: &str,
        step_index: u32,
        total_steps: u32,
        repository_path: Option<String>,
    ) -> Result<SingleFileResult> {
        let ai_manager = self.ai_manager.read().await;
        let config = ai_manager.get_config().await;

        // æ„å»ºå•æ–‡ä»¶åˆ†æçš„æç¤ºè¯
        let system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®¡æŸ¥åŠ©æ‰‹ã€‚è¯·åˆ†æç»™å®šæ–‡ä»¶çš„å˜æ›´å†…å®¹ï¼Œç”Ÿæˆç®€æ´çš„å˜æ›´æ‘˜è¦ï¼ˆ50-100å­—ï¼‰ã€‚";
        let user_prompt = format!(
            "æ–‡ä»¶è·¯å¾„: {}\n\nå˜æ›´å†…å®¹:\n{}\n\nè¯·ç”Ÿæˆè¿™ä¸ªæ–‡ä»¶å˜æ›´çš„ç®€æ´æ‘˜è¦ï¼š",
            file_path, diff_content
        );

        let request = AIRequest {
            messages: vec![
                ChatMessage {
                    role: "system".to_string(),
                    content: system_prompt.to_string(),
                },
                ChatMessage {
                    role: "user".to_string(),
                    content: user_prompt,
                },
            ],
            model: config.base.model.clone(),
            temperature: Some(0.3),
            max_tokens: Some(200),
            stream: Some(false),
        };

        let start_time = std::time::Instant::now();
        let response = ai_manager.generate_commit_message(request.clone()).await?;
        let processing_time = start_time.elapsed().as_millis() as u64;

        let tokens_used = TokenCounter::estimate_tokens(&response.content);
        let record_id = Uuid::new_v4().to_string(); // ç®€åŒ–çš„è®°å½•ID

        Ok(SingleFileResult {
            summary: FileSummary {
                file_path: file_path.to_string(),
                summary: response.content,
                tokens_used,
            },
            record_id,
        })
    }

    /// ç”Ÿæˆæœ€ç»ˆæäº¤æ¶ˆæ¯
    async fn generate_final_commit_message(
        &self,
        template_id: &str,
        file_summaries: &[FileSummary],
        branch_name: Option<String>,
        session_id: &str,
        repository_path: Option<String>,
    ) -> Result<FinalCommitResult> {
        let ai_manager = self.ai_manager.read().await;

        // æ„å»ºæ±‡æ€»çš„diffå†…å®¹
        let summary_content = file_summaries
            .iter()
            .map(|fs| format!("æ–‡ä»¶: {}\næ‘˜è¦: {}", fs.file_path, fs.summary))
            .collect::<Vec<_>>()
            .join("\n\n");

        // æ„å»ºæœ€ç»ˆæ€»ç»“çš„æç¤ºè¯
        let system_prompt = format!(
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Gitæäº¤æ¶ˆæ¯ç”ŸæˆåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ–‡ä»¶å˜æ›´æ‘˜è¦ï¼Œä½¿ç”¨{}æ¨¡æ¿ç”Ÿæˆæœ€ç»ˆçš„æäº¤æ¶ˆæ¯ã€‚",
            template_id
        );
        let user_prompt = format!(
            "ä»¥ä¸‹æ˜¯å„ä¸ªæ–‡ä»¶çš„å˜æ›´æ‘˜è¦:\n\n{}\n\nè¯·ç”Ÿæˆä¸€ä¸ªç®€æ´ã€å‡†ç¡®çš„æäº¤æ¶ˆæ¯ï¼š",
            summary_content
        );

        let request = AIRequest {
            messages: vec![
                ChatMessage {
                    role: "system".to_string(),
                    content: system_prompt,
                },
                ChatMessage {
                    role: "user".to_string(),
                    content: user_prompt,
                },
            ],
            model: ai_manager.get_config().await.base.model.clone(),
            temperature: Some(0.3),
            max_tokens: Some(500),
            stream: Some(false),
        };

        let start_time = std::time::Instant::now();
        let response = ai_manager.generate_commit_message(request).await?;
        let processing_time = start_time.elapsed().as_millis() as u64;

        let record_id = Uuid::new_v4().to_string(); // ç®€åŒ–çš„è®°å½•ID

        Ok(FinalCommitResult {
            message: response.content,
            record_id,
        })
    }

    /// è·å–æ¨¡å‹çš„æœ€å¤§tokené™åˆ¶
    async fn get_model_max_tokens(&self, model_id: &str) -> Result<Option<u32>> {
        // ç®€åŒ–å®ç°ï¼Œè¿”å›å¸¸è§æ¨¡å‹çš„tokené™åˆ¶
        let max_tokens = match model_id {
            m if m.contains("gpt-4") => Some(8192),
            m if m.contains("gpt-3.5") => Some(4096),
            m if m.contains("claude") => Some(100000),
            m if m.contains("gemini") => Some(32768),
            _ => Some(4096), // é»˜è®¤é™åˆ¶
        };

        Ok(max_tokens)
    }
}

#[derive(Debug)]
struct SingleFileResult {
    summary: FileSummary,
    record_id: String,
}

#[derive(Debug)]
struct FinalCommitResult {
    message: String,
    record_id: String,
}
