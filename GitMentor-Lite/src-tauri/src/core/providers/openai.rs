use anyhow::Result;
use async_trait::async_trait;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::time::{Duration, Instant};

use crate::core::ai_config::OpenAIConfig;
use crate::core::ai_provider::*;

/**
 * OpenAIæä¾›å•†å®ç°
 * ä½œè€…ï¼šEvilek
 * ç¼–å†™æ—¥æœŸï¼š2025-07-25
 */

#[derive(Debug, Serialize)]
struct OpenAIRequest {
    model: String,
    messages: Vec<OpenAIMessage>,
    temperature: Option<f32>,
    max_tokens: Option<u32>,
    stream: Option<bool>,
}

#[derive(Debug, Serialize)]
struct OpenAIMessage {
    role: String,
    content: String,
}

#[derive(Debug, Deserialize)]
struct OpenAIResponse {
    choices: Vec<OpenAIChoice>,
    usage: Option<OpenAIUsage>,
    model: String,
}

#[derive(Debug, Deserialize)]
struct OpenAIChoice {
    message: OpenAIResponseMessage,
    finish_reason: Option<String>,
}

#[derive(Debug, Deserialize)]
struct OpenAIResponseMessage {
    content: String,
}

#[derive(Debug, Deserialize)]
struct OpenAIUsage {
    prompt_tokens: u32,
    completion_tokens: u32,
    total_tokens: u32,
}

#[derive(Debug, Deserialize)]
struct OpenAIModelsResponse {
    data: Vec<OpenAIModelInfo>,
}

#[derive(Debug, Deserialize)]
struct OpenAIModelInfo {
    id: String,
    #[allow(dead_code)]
    object: String,
    #[allow(dead_code)]
    owned_by: String,
}

pub struct OpenAIProvider {
    client: Client,
    config: OpenAIConfig,
}

impl OpenAIProvider {
    pub fn new(config: OpenAIConfig) -> Self {
        let client = Client::builder()
            .timeout(Duration::from_secs(300)) // å¢åŠ åˆ°5åˆ†é’Ÿï¼Œé¿å…é•¿å“åº”è¢«æˆªæ–­ - Author: Evilek, Date: 2025-01-10
            .build()
            .expect("Failed to create HTTP client");

        Self { client, config }
    }

    fn get_headers(&self) -> reqwest::header::HeaderMap {
        let mut headers = reqwest::header::HeaderMap::new();
        headers.insert("Content-Type", "application/json".parse().unwrap());

        if !self.config.api_key.is_empty() {
            headers.insert(
                "Authorization",
                format!("Bearer {}", self.config.api_key).parse().unwrap(),
            );
        }

        headers
    }
}

#[async_trait]
impl AIProvider for OpenAIProvider {
    fn get_id(&self) -> &str {
        "OpenAI"
    }

    fn get_name(&self) -> &str {
        "OpenAI"
    }

    async fn generate_commit(&self, request: &AIRequest) -> Result<AIResponse> {
        // æ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨æµå¼è¯·æ±‚ä»¥é¿å…è¶…æ—¶
        let openai_request = OpenAIRequest {
            model: request.model.clone(),
            messages: request
                .messages
                .iter()
                .map(|msg| OpenAIMessage {
                    role: msg.role.clone(),
                    content: msg.content.clone(),
                })
                .collect(),
            temperature: request.temperature,
            max_tokens: request.max_tokens,
            stream: Some(true), // å¼ºåˆ¶ä½¿ç”¨æµå¼è¯·æ±‚
        };

        let url = &format!("{}/chat/completions", self.config.base_url);
        println!("ğŸ” [OpenAI] è¯·æ±‚URL: {}", url);
        println!("ğŸ” [OpenAI] è¯·æ±‚å¤´: {:?}", self.get_headers());
        println!("ğŸ” [OpenAI] è¯·æ±‚æ¨¡å‹: {} (æµå¼: æ˜¯)", openai_request.model);

        println!("ğŸ” [OpenAI] å¼€å§‹æµå¼è¯·æ±‚...");

        let mut response = self
            .client
            .post(url)
            .headers(self.get_headers())
            .json(&openai_request)
            .send()
            .await?;

        let status = response.status();
        let headers = response.headers().clone();

        println!("ğŸ” [OpenAI] æµå¼è¯·æ±‚HTTPçŠ¶æ€ç : {}", status);
        println!("ğŸ” [OpenAI] æµå¼è¯·æ±‚å“åº”å¤´: {:?}", headers);

        if !status.is_success() {
            let error_text = response.text().await?;
            println!("âŒ [OpenAI] æµå¼è¯·æ±‚é”™è¯¯: {}", error_text);
            return Err(anyhow::anyhow!("OpenAI streaming error: {}", error_text));
        }

        // è¯»å–æµå¼å“åº”
        let mut final_content = String::new();
        let mut model_name = String::new();
        let mut usage_info = None;

        while let Some(chunk) = response.chunk().await? {
            let chunk_str = String::from_utf8_lossy(&chunk);

            // è§£æSSEæ ¼å¼
            for line in chunk_str.lines() {
                if line.starts_with("data: ") {
                    let data = &line[6..]; // ç§»é™¤ "data: " å‰ç¼€

                    if data.trim() == "[DONE]" {
                        println!("ğŸ” [OpenAI] æµå¼å“åº”å®Œæˆ");
                        continue;
                    }

                    if let Ok(chunk_data) = serde_json::from_str::<serde_json::Value>(data) {
                        // ä¿å­˜æ¨¡å‹åç§°
                        if let Some(model) = chunk_data.get("model").and_then(|m| m.as_str()) {
                            model_name = model.to_string();
                        }

                        // å¤„ç†choicesæ•°ç»„
                        if let Some(choices) = chunk_data.get("choices").and_then(|c| c.as_array())
                        {
                            if let Some(choice) = choices.first() {
                                // å¤„ç†deltaå†…å®¹
                                if let Some(delta) = choice.get("delta") {
                                    // ç´¯ç§¯å†…å®¹
                                    if let Some(content) =
                                        delta.get("content").and_then(|c| c.as_str())
                                    {
                                        final_content.push_str(content);
                                        // å®æ—¶æ‰“å°æ¥æ”¶åˆ°çš„å†…å®¹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                                        print!("{}", content);
                                        std::io::Write::flush(&mut std::io::stdout()).unwrap();
                                    }
                                }

                                // å¤„ç†ä½¿ç”¨é‡ä¿¡æ¯ï¼ˆé€šå¸¸åœ¨æœ€åä¸€ä¸ªchunkä¸­ï¼‰
                                if let Some(usage) = chunk_data.get("usage") {
                                    usage_info = Some(usage.clone());
                                }
                            }
                        }
                    }
                }
            }
        }

        println!(
            "\nğŸ” [OpenAI] æµå¼æ¥æ”¶å®Œæˆï¼Œæ€»é•¿åº¦: {}",
            final_content.len()
        );

        // å¦‚æœæ²¡æœ‰æ”¶åˆ°ä»»ä½•å†…å®¹ï¼Œè¿”å›é”™è¯¯
        if final_content.is_empty() {
            return Err(anyhow::anyhow!(
                "No content received from streaming response"
            ));
        }

        // é¦–å…ˆä½¿ç”¨ ReasoningParser åˆ†ç¦»æ€è€ƒå†…å®¹å’Œå®é™…å†…å®¹
        let (actual_content, reasoning_content) =
            crate::core::ai_provider::ReasoningParser::parse_content(&final_content);

        // å¦‚æœæœ‰æ€è€ƒå†…å®¹ï¼Œæ‰“å°æ—¥å¿—
        if let Some(ref reasoning) = reasoning_content {
            println!("ğŸ” [DEBUG] æå–åˆ°æ€è€ƒå†…å®¹ï¼Œé•¿åº¦: {}", reasoning.len());
        }

        // ç„¶åå¯¹å®é™…å†…å®¹è¿›è¡Œè¿›ä¸€æ­¥æ¸…ç†
        use crate::core::response_cleaner::ResponseCleaner;
        let cleaned_content = ResponseCleaner::clean_commit_message(&actual_content);

        println!(
            "ğŸ” [DEBUG] åŸå§‹å“åº”é•¿åº¦: {}, ç§»é™¤æ€è€ƒåé•¿åº¦: {}, æ¸…ç†åé•¿åº¦: {}",
            final_content.len(),
            actual_content.len(),
            cleaned_content.len()
        );

        // æ„é€ AIå“åº”ï¼Œç›´æ¥ä½¿ç”¨å·²æ¸…ç†çš„å†…å®¹
        Ok(AIResponse {
            content: cleaned_content,
            reasoning_content,
            model: model_name,
            usage: usage_info.map(|u| TokenUsage {
                prompt_tokens: u.get("prompt_tokens").and_then(|v| v.as_u64()).unwrap_or(0) as u32,
                completion_tokens: u
                    .get("completion_tokens")
                    .and_then(|v| v.as_u64())
                    .unwrap_or(0) as u32,
                total_tokens: u.get("total_tokens").and_then(|v| v.as_u64()).unwrap_or(0) as u32,
            }),
            finish_reason: Some("stop".to_string()),
        })
    }

    async fn get_models(&self) -> Result<Vec<AIModel>> {
        let url = &format!("{}/models", self.config.base_url);

        let response = self
            .client
            .get(url)
            .headers(self.get_headers())
            .send()
            .await?;

        if !response.status().is_success() {
            let error_text = response.text().await?;
            return Err(anyhow::anyhow!("Failed to get models: {}", error_text));
        }

        // æ£€æŸ¥å“åº”å†…å®¹æ˜¯å¦ä¸ºç©º
        let response_text = response.text().await?;
        if response_text.trim().is_empty() {
            return Err(anyhow::anyhow!(
                "OpenAI API returned empty response for models"
            ));
        }

        // è§£æJSONå“åº”
        let models_response: OpenAIModelsResponse =
            serde_json::from_str(&response_text).map_err(|e| {
                anyhow::anyhow!(
                    "Failed to parse OpenAI models response: {}. Response text: {}",
                    e,
                    if response_text.len() > 200 {
                        &response_text[..200]
                    } else {
                        &response_text
                    }
                )
            })?;

        let models: Vec<AIModel> = models_response
            .data
            .into_iter()
            .map(|model| AIModel {
                id: model.id.clone(),
                name: model.id,
                max_tokens: Some(4096), // é»˜è®¤å€¼ï¼Œå®é™…åº”æ ¹æ®æ¨¡å‹è°ƒæ•´
                provider: "OpenAI".to_string(),
                default: Some(false),
                hidden: Some(false),
                capabilities: Some(ModelCapabilities {
                    streaming: Some(true),
                    function_calling: Some(true),
                }),
                cost: None,
            })
            .collect();

        Ok(models)
    }

    async fn test_connection(&self) -> Result<ConnectionTestResult> {
        let start_time = Instant::now();

        match self.get_models().await {
            Ok(models) => {
                let latency = start_time.elapsed().as_millis() as u64;
                Ok(ConnectionTestResult {
                    success: true,
                    message: "è¿æ¥æˆåŠŸ".to_string(),
                    latency_ms: Some(latency),
                    model_count: Some(models.len()),
                })
            }
            Err(e) => Ok(ConnectionTestResult {
                success: false,
                message: format!("è¿æ¥å¤±è´¥: {}", e),
                latency_ms: None,
                model_count: None,
            }),
        }
    }

    async fn is_available(&self) -> bool {
        !self.config.api_key.is_empty() && !self.config.base_url.is_empty()
    }

    async fn refresh_models(&self) -> Result<Vec<AIModel>> {
        self.get_models().await
    }
}
